{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4Ipciid2Uz7X7fkLS4fW0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk6Wx9c5jY8F",
        "outputId": "713551fa-be3b-44ab-f847-6fb1e372db5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/vishakhdapat/imdb-movie-reviews?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25.7M/25.7M [00:01<00:00, 16.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/vishakhdapat/imdb-movie-reviews/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"vishakhdapat/imdb-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2k_hDKZMoyfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5e7511-9a1c-421c-fa22-0be93e572ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(dataset):\n",
        "  dataset = dataset.lower()\n",
        "  dataset = re.sub(r\"\\s+(?=[^w\\s])|(?<=[^w\\s])\\s+\",\"\",dataset)\n",
        "  dataset = re.sub(r\"\\d+\",\" \",dataset)\n",
        "  return dataset\n",
        "def tokenizer(dataset):\n",
        "  dataset = word_tokenize(dataset)\n",
        "  return dataset\n",
        "def delete_stopwords(dataset):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  if all(word in stop_words for word in dataset):\n",
        "      return dataset\n",
        "  dataset = [word for word in dataset if word not in stop_words]\n",
        "  return dataset\n",
        "\n",
        "def full_preprocessor(dataset):\n",
        "  dataset = cleaner(dataset)\n",
        "  dataset = tokenizer(dataset)\n",
        "  dataset = delete_stopwords(dataset)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "data=pd.read_csv(path+\"/IMDB Dataset.csv\")\n",
        "\n",
        "data[\"review\"] = data[\"review\"].apply(full_preprocessor)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"review\"], data[\"sentiment\"], test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(x)))\n",
        "X_test_vec = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))\n",
        "\n",
        "\n",
        "y_train = y_train.apply(lambda x: 1 if x==\"positive\" else 0)\n",
        "y_test = y_test.apply(lambda x: 1 if x==\"positive\" else 0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AWziiGsXj4wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LogisticRegression()\n",
        "model.fit(X_train_vec,y_train)\n",
        "model.score(X_test_vec,y_test)\n"
      ],
      "metadata": {
        "id": "Y4_w-lQBn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52296b91-f1d3-41c6-8951-9a36abdc66f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deleter(dataset):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  dataset = [word for word in dataset if word not in stop_words]\n",
        "  return dataset\n",
        "def preprocessor_for_real_data(dataset):\n",
        "  dataset = cleaner(x_real)\n",
        "  dataset = tokenizer(dataset)\n",
        "  dataset = deleter(dataset)\n",
        "  return dataset\n",
        "\n",
        "x_real= \"The variety of tasks kept it interesting, and the real-world feel of the problems made the math more intuitive.\"\n",
        "x_real=preprocessor_for_real_data(x_real)\n",
        "x_real_str = ' '.join(x_real)\n",
        "x_real_vec = vectorizer.transform([x_real_str])\n",
        "pred=model.predict(x_real_vec)\n",
        "print(pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "sDnPJbUJEKKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c063c218-4d2e-4fdb-b851-9733f56cf481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8ql2Px5zDPP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}